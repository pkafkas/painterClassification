{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440d6285-3f66-430c-9786-7d489bb75b60",
   "metadata": {},
   "source": [
    "# Machine Learning: Paintings Classification Problem\n",
    "\n",
    "## 0. Introduction: \n",
    "In this project we try to accurately predict the artist that painted a given painting. To this end, we train three different classifiers and we test them on a dataset compiled of paintings from four different painters: Salvador Dali, Claude Monet, Pablo Picasso, H. Rembrandt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9976c-6628-4074-9755-b54057a9d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage.filters import sobel\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import joblib\n",
    "import urllib.request\n",
    "import validators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f79252-ec88-46a5-a0da-18611b1984d8",
   "metadata": {},
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "### 1.1 Convert Images\n",
    "\n",
    "This code segment contains all methods that process the original images by transforming them to a given size (SIZE var.) and then transforming them to greyscale. After that, they are stored in an numpy array. There is also the option to replace the original images with the resized ones, for easier manipulation at a later time and smaller storage space. Finaly, the data_process method is used for preparing the data for the training of the model, while the image_process method takes as argument only a single image and is used later for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d997a-06de-4535-bd4d-ce428df44c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(string_path):\n",
    "    SIZE = 64  # set the image size convertion \n",
    "\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    for dir_path in glob.glob(string_path):\n",
    "        #print(dir_path)\n",
    "        label = dir_path.split('\\\\')[-1]\n",
    "        #print(label)\n",
    "\n",
    "        k = 0\n",
    "        for img_path in glob.glob(os.path.join(dir_path, '*.jpg')):\n",
    "            if k < 500:\n",
    "                #print(img_path)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.resize(img, (SIZE,SIZE))\n",
    "                # cv2.imwrite(img_path, img)   # Use this to replace original images with resized ones\n",
    "                # convert to greyscale\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                train_images.append(img)\n",
    "                train_labels.append(label)\n",
    "            k+=1\n",
    "\n",
    "    train_images = np.array(train_images)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    return train_images, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceee14a-5a5d-4b8d-a17b-c239954d2704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data process\n",
    "\n",
    "# Data for training/testing/validation\n",
    "x_train, train_labels = data_process('RawData/train/*')\n",
    "x_train = x_train / 255.0\n",
    "x_train.shape\n",
    "\n",
    "# Data for demonstration\n",
    "x_demo, demo_labels = data_process('RawData/demo/*')\n",
    "x_demo = x_demo / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f56acd-b82f-4f69-b01e-4d85832571bf",
   "metadata": {},
   "source": [
    "### 1.2 Extract features from the images & create the future matrix\n",
    "\n",
    "This segment contains the functions that are used to extract features from the images. The input is an numpy array containing the original images (as it is transformed and prepared from the previous functions). The feature_extractor() method takes as input the numpy array and calculates 4 different Gabor features per image, as well as hog features for each image and its pixel values. Its output is a pandas dataframe with each feature as a column (plus a hog_size var that is used later for processing the final feature matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1672c6-0c83-44a3-ae41-889c63cfc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction function\n",
    "def feature_extractor(dataset):\n",
    "    SIZE = 64\n",
    "    image_dataset = pd.DataFrame()\n",
    "    for image in range(dataset.shape[0]):\n",
    "        df = pd.DataFrame()\n",
    "        input_img = dataset[image, :, :]  # one more dimension [:] if I have color\n",
    "        img = input_img\n",
    "        \n",
    "        # Pixel values\n",
    "        pixel_values = img.reshape(-1)\n",
    "        df['Pixel_Value'] = pixel_values\n",
    "        #df['Image_Name'] = image\n",
    "        \n",
    "        # Gabor\n",
    "        num = 1\n",
    "        kernels = []\n",
    "        for theta in range(2):\n",
    "            theta = theta / 4. * np.pi\n",
    "            for sigma in range(1,3):  # range(1,3) default\n",
    "                lamda = np.pi / 4.\n",
    "                gamma = 0.5\n",
    "                gabor_label = 'Gabor' + str(num)\n",
    "                ksize = 9\n",
    "                kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)    \n",
    "                kernels.append(kernel)\n",
    "                \n",
    "                fimg = cv2.filter2D(img, cv2.CV_8UC3, kernel)\n",
    "                filtered_img = fimg.reshape(-1)\n",
    "                df[gabor_label] = filtered_img  #Labels columns as Gabor1, Gabor2, etc.\n",
    "#                print(gabor_label, ': theta=', theta, ': sigma=', sigma, ': lamda=', lamda, ': gamma=', gamma)\n",
    "                num += 1  #Increment for gabor column label\n",
    "    \n",
    "        # Hog\n",
    "        resized_img = cv2.resize(img, (64,128))\n",
    "        fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                    cells_per_block=(2, 2), visualize=True)\n",
    "        \n",
    "        hog_size = fd.shape\n",
    "        d_zeroes = pd.DataFrame(np.zeros((SIZE*SIZE, 1)))\n",
    "        d_hog = pd.DataFrame(fd)\n",
    "        df2 = d_zeroes.iloc[3780:,:]\n",
    "        df3 = pd.concat([d_hog,df2],axis=0, ignore_index=True)\n",
    "        df['hog'] = df3\n",
    "\n",
    "            \n",
    "        image_dataset = pd.concat([image_dataset, df],axis=0) \n",
    "    return image_dataset, hog_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988628a-881f-44da-9454-ec310ad2e574",
   "metadata": {},
   "source": [
    "This segment contains the method that is used to create the final feature matrix. It takes as input the pandas dataframe previously created by the feature_extractor method and it reshapes it by flattening the columns and concatinating them. The end result is a pandas dataframe that contains the extracted features as a row for each image. That is, each row is essentialy the feature vector corresponding to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe457f75-d970-4f22-b86b-d91841d76c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(x_train, image_features):\n",
    "    SIZE = 64\n",
    "    pixels_index= SIZE*SIZE \n",
    "    image_numbers = x_train.shape[0]\n",
    "\n",
    "    feature_matrix = pd.DataFrame()\n",
    "    for i in range(0, image_numbers):\n",
    "        new = pd.DataFrame()\n",
    "        temp_hog = pd.DataFrame()\n",
    "        df1 = image_features.iloc[:pixels_index,:]\n",
    "        df2 = image_features.iloc[pixels_index:,:]\n",
    "\n",
    "        new = pd.concat([new, df1['Pixel_Value']],axis=0, ignore_index=True) \n",
    "        new = pd.concat([new, df1['Gabor1']],axis=0, ignore_index=True) \n",
    "        new = pd.concat([new, df1['Gabor2']],axis=0, ignore_index=True) \n",
    "        new = pd.concat([new, df1['Gabor3']],axis=0, ignore_index=True) \n",
    "        new = pd.concat([new, df1['Gabor4']],axis=0, ignore_index=True)\n",
    "\n",
    "        temp_hog = df1['hog']\n",
    "        temp_hog = temp_hog[:3780]   # TODO: add hog size VAR\n",
    "\n",
    "        new = pd.concat([new, temp_hog],axis=0, ignore_index=True)\n",
    "        new = new.T\n",
    "\n",
    "        feature_matrix = pd.concat([feature_matrix, new], axis=0, ignore_index=True) \n",
    "        image_features = df2\n",
    "        new = pd.DataFrame(None)\n",
    "        temp_hog = pd.DataFrame(None)\n",
    "    \n",
    "    return feature_matrix\n",
    "#    feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4ad9b-2abf-4fcc-9f14-98dcc5dd7987",
   "metadata": {},
   "source": [
    "Use the above functions to convert the .jpg images in train/demo folders to a feature matrix corresponding to each, then export the feature matrix to .csv files for future use. Export also a PCA reduced feature matrix for visualization. In total, after we run the next 4 cells we should have generated four distinct files in our main project directory: \n",
    "1. feature_matrix.csv\n",
    "2. feature_matrix_pca.csv \n",
    "3. feature_matrix_demo.csv\n",
    "4. feature_matrix_demo_pca.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac7051-521e-4994-8b8f-24a026c8a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for training\n",
    "\n",
    "# feature extraction\n",
    "image_features, hog_size = feature_extractor(x_train)\n",
    "# feature matrix creation\n",
    "feature_matrix = create_feature_matrix(x_train, image_features)\n",
    "\n",
    "# pandas dataframe creation\n",
    "feature_matrix['painter'] = train_labels\n",
    "feature_matrix.reset_index(drop=True)\n",
    "\n",
    "### Export Feature Matrix to .csv file\n",
    "feature_matrix.to_csv('feature_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d960d-f54f-435b-8082-2fe7e2f6896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ea866-d7be-45c4-9776-9793cd6ac659",
   "metadata": {},
   "source": [
    "### 1.3 Train Data PCA & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c09ef-8a8c-4c15-8dfb-f5ac7c25d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for visualization   ***** Do this only once, then load the dataset\n",
    "\n",
    "feature_matrix = pd.read_csv('feature_matrix.csv')  # ****LOAD if you have allready created it from previous section\n",
    "\n",
    "X = feature_matrix.drop(columns = 'painter')\n",
    "y = feature_matrix['painter']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "\n",
    "feature_matrix_pca = pd.DataFrame(data = X_pca, columns = ['pca_1', 'pca_2', 'pca_3'])\n",
    "feature_matrix_pca['painter'] = y\n",
    "\n",
    "feature_matrix_pca.to_csv('feature_matrix_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd01f7-35ad-46c4-9f12-66b2eb41f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the pca reduced image of the dataset:\n",
    "fig = px.scatter_3d(feature_matrix_pca, x='pca_1', y='pca_2', z='pca_3',\n",
    "              color='painter')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b42cf-bfc0-437a-9614-45ba558556a3",
   "metadata": {},
   "source": [
    "### 1.4 Create Demo Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea6c53-65d5-464a-b0c7-a7bdc57ea4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for DEMO\n",
    "\n",
    "# feature extraction\n",
    "image_features, hog_size = feature_extractor(x_demo)\n",
    "# feature matrix creation\n",
    "feature_matrix_demo = create_feature_matrix(x_demo, image_features)\n",
    "\n",
    "# pandas dataframe creation\n",
    "feature_matrix_demo['painter'] = demo_labels\n",
    "feature_matrix_demo.reset_index(drop=True)\n",
    "\n",
    "### Export Feature Matrix to .csv file\n",
    "feature_matrix_demo.to_csv('feature_matrix_demo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07864e58-5ce8-4084-a211-1dc1bde47bf4",
   "metadata": {},
   "source": [
    "### 1.5 Demo Data PCA & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a1de5-4118-42b6-ad96-9717e08384ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA for demo visualization\n",
    "\n",
    "# feature_matrix_demo = pd.read_csv('feature_matrix_demo.csv')  ****LOAD if you have allready created it from previous section\n",
    "\n",
    "X = feature_matrix_demo.drop(columns = 'painter')\n",
    "y = feature_matrix_demo['painter']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "\n",
    "feature_matrix_pca = pd.DataFrame(data = X_pca, columns = ['pca_1', 'pca_2', 'pca_3'])\n",
    "feature_matrix_pca['painter'] = y\n",
    "\n",
    "feature_matrix_pca.to_csv('feature_matrix_demo_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32789d8d-679e-41a0-a241-e2f335aa407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the pca reduced image of the dataset:\n",
    "fig = px.scatter_3d(feature_matrix_pca, x='pca_1', y='pca_2', z='pca_3',\n",
    "              color='painter')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b0070-e168-4318-8398-aabcf372105a",
   "metadata": {},
   "source": [
    "### 1.6 Feature selection\n",
    "\n",
    "In this section we import the feature matrix that we created and we apply a selectKBest method in order to rank the features and select the best. We use a basic random forest classifier as benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275d912-c7ca-4570-989e-1456821934f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = pd.read_csv('feature_matrix.csv')   # LOAD if you have allready created it from previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d482ac4-122d-4858-a85d-8cb14cfe6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_matrix.drop(columns = 'painter')\n",
    "y = feature_matrix['painter']\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform (X_test)\n",
    "\n",
    "score_features = []\n",
    "\n",
    "for i in range(1000, len(X.iloc[0]), 1000):\n",
    "    X_new = SelectKBest(chi2, k=i).fit_transform(X, y)\n",
    "    rf = RandomForestClassifier()\n",
    "    scores = cross_val_score(rf, X_new, y, cv=3)\n",
    "    scores = round(scores.mean(),4)*100\n",
    "    score_features.append([scores, i])\n",
    "    print(f'Mean score for {i} features is {scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c76715-b0c8-4070-ab80-4dc59bdbcd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.max(score_features))\n",
    "score_features = np.array(score_features)\n",
    "score_feature = np.round(score_features)\n",
    "print(score_features[:,0])\n",
    "max = np.max(score_features[:,0])\n",
    "#print(max)\n",
    "x =np.where(score_features == max)\n",
    "#print(x[0])\n",
    "bestK = int(score_features[x[0], 1])\n",
    "#print(score_features[x[0], 1])\n",
    "#print(bestK)\n",
    "\n",
    "print(f\"The best feature number that yielded the highest accuracy based on the selectKBest: {bestK}\")\n",
    "select = SelectKBest(score_func=chi2, k=bestK)\n",
    "z = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac7c03-119a-4061-ae87-e58af2e44ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_kbest = pd.DataFrame(data = z)\n",
    "feature_matrix_kbest['painter'] = y\n",
    "feature_matrix_kbest.tail()\n",
    "feature_matrix_kbest.to_csv('feature_matrix_kbest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91871335-3718-4882-931f-8f98f6289f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_kbest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afacae8-e921-4dc3-aa1f-639a12d556de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=score_features[:,0], y=score_features[:,1], labels={'x':'cross-val score (cv=3)', 'y':'number of features'}) # override keyword names with labels\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
